{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling of Video Game Review Information from Steam Website\n",
    "In this initial step I'll extract video game information and corresponding user reviews through Valve's Steam digital distribution company and supporting websites.\n",
    "\n",
    "The data is described at https://partner.steamgames.com/doc/store/getreviews and https://www.steamspy.com/about, and extracted using the steamreviews API from https://pypi.org/project/steamreviews/ and code from Nik Davis at https://nik-davis.github.io/posts/2019/steam-data-collection/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import json\n",
    "import os.path\n",
    "import time\n",
    "import datetime as dt\n",
    "import re\n",
    "import csv\n",
    "import statistics\n",
    "\n",
    "# third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import steamreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your personal folder\n",
    "path_steam = \"C:/users/ggibs/steam_game_recommender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs and subfolders\n",
    "steam_url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "steamspy_url = \"https://steamspy.com/api.php\"\n",
    "\n",
    "path_raw = os.path.join(path_steam, \"data/raw/\")\n",
    "path_interim = os.path.join(path_steam, \"data/interim/\")\n",
    "path_external = os.path.join(path_steam, \"data/external/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url, parameters=None):\n",
    "    \"\"\"Return json-formatted response of a get request using optional parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "        parameters to pass as part of get request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url=url, params=parameters)\n",
    "    except SSLError as s:\n",
    "        print('SSL Error:', s)\n",
    "        \n",
    "        for i in range(5, 0, -1):\n",
    "            print('\\rWaiting... ({})'.format(i), end='')\n",
    "            time.sleep(1)\n",
    "        print('\\rRetrying.' + ' '*10)\n",
    "        \n",
    "        # recursively try again\n",
    "        return get_request(url, parameters)\n",
    "    \n",
    "    if response:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # response is none usually means too many requests. Wait and try again \n",
    "        print('No response, waiting 10 seconds...')\n",
    "        time.sleep(10)\n",
    "        print('Retrying.')\n",
    "        return get_request(url, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to track records completed\n",
    "\n",
    "def reset_index(download_path, index_filename):\n",
    "    \"\"\"Reset index in file to 0.\"\"\"\n",
    "    rel_path = os.path.join(download_path, index_filename)\n",
    "    \n",
    "    with open(rel_path, 'w') as f:\n",
    "        print(0, file=f)\n",
    "        \n",
    "\n",
    "def get_index(download_path, index_filename):\n",
    "    \"\"\"Retrieve index from file, returning 0 if file not found.\"\"\"\n",
    "    try:\n",
    "        rel_path = os.path.join(download_path, index_filename)\n",
    "\n",
    "        with open(rel_path, 'r') as f:\n",
    "            index = int(f.readline())\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        index = 0\n",
    "        \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_file(download_path, filename, index, columns):\n",
    "    \"\"\"Create file and write headers if index is 0.\"\"\"\n",
    "    if index == 0:\n",
    "        rel_path = os.path.join(download_path, filename)\n",
    "\n",
    "        with open(rel_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steam_request(appid, name):\n",
    "    \"\"\"Unique parser to handle data from Steam Store API.\n",
    "    \n",
    "    Returns : json formatted data (dict-like)\n",
    "    \"\"\"\n",
    "    url = steam_url\n",
    "    parameters = {\"appids\": appid}\n",
    "    \n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    json_app_data = json_data[str(appid)]\n",
    "    \n",
    "    if json_app_data['success']:\n",
    "        data = json_app_data['data']\n",
    "    else:\n",
    "        data = {'name': name, 'steam_appid': appid}\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steamspy_request(appid, name):\n",
    "    \"\"\"Parser to handle SteamSpy API data.\"\"\"\n",
    "    url = steamspy_url\n",
    "    parameters = {\"request\": \"appdetails\", \"appid\": appid}\n",
    "    \n",
    "    json_data = get_request(url, parameters)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_data(start, stop, parser, pause):\n",
    "    \"\"\"Return list of app data generated from parser.\n",
    "    \n",
    "    parser : function to handle request\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "    \n",
    "    # iterate through each row of app_list, confined by start and stop\n",
    "    for index, row in app_list[start:stop].iterrows():\n",
    "        print('Current index: {}'.format(index), end='\\r')\n",
    "        \n",
    "        appid = row['appid']\n",
    "        name = row['name']\n",
    "\n",
    "        # retrive app data for a row, handled by supplied parser, and append to list\n",
    "        data = parser(appid, name)\n",
    "        app_data.append(data)\n",
    "\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "    \n",
    "    return app_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
    "                    columns, begin=0, end=-1, batchsize=100, pause=1):\n",
    "    \"\"\"Process app data in batches, writing directly to file.\n",
    "    \n",
    "    parser : custom function to format request\n",
    "    app_list : dataframe of appid and name\n",
    "    download_path : path to store data\n",
    "    data_filename : filename to save app data\n",
    "    index_filename : filename to store highest index written\n",
    "    columns : column names for file\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    begin : starting index (get from index_filename, default 0)\n",
    "    end : index to finish (defaults to end of app_list)\n",
    "    batchsize : number of apps to write in each batch (default 100)\n",
    "    pause : time to wait after each api request (defualt 1)\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    print('Starting at index {}:\\n'.format(begin))\n",
    "    \n",
    "    # by default, process all apps in app_list\n",
    "    if end == -1:\n",
    "        end = len(app_list) + 1\n",
    "    \n",
    "    # generate array of batch begin and end points\n",
    "    batches = np.arange(begin, end, batchsize)\n",
    "    batches = np.append(batches, end)\n",
    "    \n",
    "    apps_written = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches[i+1]\n",
    "        \n",
    "        app_data = get_app_data(start, stop, parser, pause)\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        # writing app data to file\n",
    "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            \n",
    "            for j in range(3,0,-1):\n",
    "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            writer.writerows(app_data)\n",
    "            print('\\rExported lines {}-{} to {}.'.format(start, stop-1, data_filename), end=' ')\n",
    "            \n",
    "        apps_written += len(app_data)\n",
    "        \n",
    "        idx_path = os.path.join(download_path, index_filename)\n",
    "        \n",
    "        # writing last index to file\n",
    "        with open(idx_path, 'w') as f:\n",
    "            index = stop\n",
    "            print(index, file=f)\n",
    "            \n",
    "        # logging time taken\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        batch_times.append(time_taken)\n",
    "        mean_time = statistics.mean(batch_times)\n",
    "        \n",
    "        est_remaining = (len(batches) - i - 2) * mean_time\n",
    "        \n",
    "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
    "        time_td = dt.timedelta(seconds=round(time_taken))\n",
    "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
    "        \n",
    "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
    "            \n",
    "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game List and Game Information from Steam and SteamSpy\n",
    "### Columns can duplicate between sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce list of games\n",
    "url = steamspy_url\n",
    "parameters = {\"request\": \"all\"}\n",
    "\n",
    "# request 'all' from steam spy and parse into dataframe\n",
    "json_data = get_request(url, parameters=parameters)\n",
    "steam_spy_all = pd.DataFrame.from_dict(json_data, orient='index')\n",
    "\n",
    "# generate sorted app_list from steamspy data\n",
    "app_list = steam_spy_all[['appid', 'name']].sort_values('appid').reset_index(drop=True)\n",
    "\n",
    "# export, comment out after first run\n",
    "app_list.to_csv(os.path.join(path_external, app_list.csv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from stored csv\n",
    "app_list = pd.read_csv(os.path.join(path_external, \"app_list.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set files and columns\n",
    "download_path = path_external\n",
    "steamspy_data = 'steamspy_data.csv'\n",
    "steamspy_index = 'steamspy_index.txt'\n",
    "\n",
    "steamspy_columns = [\n",
    "    'appid', 'name', 'developer', 'publisher', 'score_rank', 'positive',\n",
    "    'negative', 'userscore', 'owners', 'average_forever', 'average_2weeks',\n",
    "    'median_forever', 'median_2weeks', 'price', 'initialprice', 'discount',\n",
    "    'languages', 'genre', 'ccu', 'tags'\n",
    "]\n",
    "\n",
    "reset_index(download_path, steamspy_index)\n",
    "index = get_index(download_path, steamspy_index)\n",
    "\n",
    "# Wipe data file if index is 0\n",
    "prepare_data_file(download_path, steamspy_data, index, steamspy_columns)\n",
    "\n",
    "process_batches(\n",
    "    parser=parse_steamspy_request,\n",
    "    app_list=app_list,\n",
    "    download_path=download_path, \n",
    "    data_filename=steamspy_data,\n",
    "    index_filename=steamspy_index,\n",
    "    columns=steamspy_columns,\n",
    "    begin=index,\n",
    "#    end=20,\n",
    "#    batchsize=5,\n",
    "#    pause=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "\n",
    "Starting at index 0:\n",
    "\n",
    "Exported lines 0-99 to steamspy_data.csv. Batch 0 time: 0:02:16 (avg: 0:02:16, remaining: 0:22:37)  \n",
    "Exported lines 100-199 to steamspy_data.csv. Batch 1 time: 0:02:14 (avg: 0:02:15, remaining: 0:20:12)  \n",
    "Exported lines 200-299 to steamspy_data.csv. Batch 2 time: 0:02:20 (avg: 0:02:17, remaining: 0:18:12)  \n",
    "Exported lines 300-399 to steamspy_data.csv. Batch 3 time: 0:02:21 (avg: 0:02:18, remaining: 0:16:03)  \n",
    "Exported lines 400-499 to steamspy_data.csv. Batch 4 time: 0:02:38 (avg: 0:02:22, remaining: 0:14:09)  \n",
    "Exported lines 500-599 to steamspy_data.csv. Batch 5 time: 0:02:16 (avg: 0:02:21, remaining: 0:11:43)  \n",
    "Exported lines 600-699 to steamspy_data.csv. Batch 6 time: 0:02:22 (avg: 0:02:21, remaining: 0:09:23)  \n",
    "Exported lines 700-799 to steamspy_data.csv. Batch 7 time: 0:02:23 (avg: 0:02:21, remaining: 0:07:03)  \n",
    "Exported lines 800-899 to steamspy_data.csv. Batch 8 time: 0:02:14 (avg: 0:02:20, remaining: 0:04:41)  \n",
    "Exported lines 900-999 to steamspy_data.csv. Batch 9 time: 0:02:14 (avg: 0:02:20, remaining: 0:02:20)  \n",
    "Exported lines 1000-1000 to steamspy_data.csv. Batch 10 time: 0:00:02 (avg: 0:02:07, remaining: 0:00:00)  \n",
    "\n",
    "Processing batches complete. 1000 apps written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file parameters\n",
    "download_path = path_external\n",
    "steam_app_data = 'steam_app_data.csv'\n",
    "steam_index = 'steam_index.txt'\n",
    "\n",
    "steam_columns = [\n",
    "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors'\n",
    "]\n",
    "\n",
    "# Store highest index so can continue across sessions)\n",
    "reset_index(download_path, steam_index)\n",
    "\n",
    "# Retrieve last index downloaded from file\n",
    "index = get_index(download_path, steam_index)\n",
    "\n",
    "# Wipe or create data file and write headers if index is 0\n",
    "prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
    "\n",
    "# Set batch parameters, remove batchsize and end to run through entire app list\n",
    "process_batches(\n",
    "    parser=parse_steam_request,\n",
    "    app_list=app_list,\n",
    "    download_path=download_path,\n",
    "    data_filename=steam_app_data,\n",
    "    index_filename=steam_index,\n",
    "    columns=steam_columns,\n",
    "    begin=index,\n",
    "#    end=30,\n",
    "#    batchsize=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constrain API query of reviews\n",
    "# reference: https://partner.steamgames.com/doc/store/getreviews\n",
    "request_params = dict()\n",
    "request_params['language'] = 'english'\n",
    "request_params['purchase_type'] = 'steam'  # customer paid for game\n",
    "request_params['day_range'] = '1095'  # the past three years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to list\n",
    "list_of_games_id = app_list.appid.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 query limit, then 5 min break, took 19 hours to complete\n",
    "os.chdir(path_raw)\n",
    "steamreviews.download_reviews_for_app_id_batch(list_of_games_id, chosen_request_params=request_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_columns = [\n",
    "    'recommendationid', 'language', 'review', 'timestamp_created', 'timestamp_updated', 'voted_up', 'votes_up', 'votes_funny',\n",
    "    'weighted_vote_score', 'comment_count', 'steam_purchase', 'received_for_free', 'written_during_early_access', \n",
    "    'author.steamid', 'author.num_games_owned', 'author.num_reviews', 'author.playtime_forever', 'author.playtime_last_two_weeks',\n",
    "    'author.playtime_at_review', 'author.last_played', 'timestamp_dev_responded', 'developer_response', 'appid'\n",
    "]\n",
    "review_df = pd.DataFrame(columns=review_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each game downloaded, open review json, flatten nested column Author, and append to dataframe\n",
    "for file in os.listdir():\n",
    "    print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "    f = open(file)\n",
    "    game_num = re.findall(\"review_(\\d+).json\", file)\n",
    "    print(game_num)\n",
    "    review_dict = json.load(f)\n",
    "    if review_dict['reviews']:    \n",
    "        for key in review_dict['reviews']:\n",
    "            df = pd.json_normalize(review_dict['reviews'][key])\n",
    "            df['appid'] = game_num\n",
    "            review_df = review_df.append(df)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output with each game ID\n",
    "12:37:18  \n",
    "['599140']  \n",
    "12:38:06  \n",
    "['601150']  \n",
    "12:39:53  \n",
    "['601510']  \n",
    "12:39:53  \n",
    "['611500']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application ID and Recommendation ID are the keys\n",
    "review_df.set_index(['appid', 'recommendationid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataframe to csv\n",
    "reviews_df.to_csv(os.path.join(path_interim ,\"reviews.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
